# LLM Configuration (Ollama/OpenAI-compatible)
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=ollama

# Opik Observability Configuration
# For cloud Opik: Set OPIK_API_KEY and use default endpoint
# For local Opik: Set OPIK_ENDPOINT to your local instance (e.g., http://localhost:5173) and leave OPIK_API_KEY empty
OPIK_API_KEY=your_opik_api_key_here
OPIK_ENDPOINT=https://api.opik.ai
# OPIK_ENDPOINT=http://localhost:5173  # Uncomment for local Opik

# MCP Server Configuration (External MCP Servers)
WEATHER_MCP_URL=http://weather.fortinet.demo:3000/sse
BOOKING_MCP_URL=http://booking.fortinet.demo:8787/mcp
MCP_API_KEY=XSgKyxLZjm-oSNztCPrLfdW1U3yepHjsIePCsPXGDdk

# Frontend Configuration
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
NEXT_PUBLIC_OPIK_ENDPOINT=https://api.opik.ai
# NEXT_PUBLIC_OPIK_ENDPOINT=http://localhost:5173  # Uncomment for local Opik